{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, Flatten, Dense\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# from evaluation import evaluate\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01matexit\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Logger'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "# from evaluation import evaluate\n",
    "from Logger import Logger\n",
    "import argparse\n",
    "import atexit\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", render_mode='human')\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "q_values = np.zeros((state_size[0], state_size[1], action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_state(state):\n",
    "\n",
    "    image_array = state[0]\n",
    "\n",
    "    #crop and resize the image\n",
    "    image = image_array[1:176:2, ::2]\n",
    "    \n",
    "    #convert the image to greyscale\n",
    "    image = image.mean(axis=2)\n",
    "\n",
    "    #improve image contrast\n",
    "    image[image==color] = 0\n",
    "\n",
    "    #normalize the image\n",
    "    image = (image - 128) / 128 - 1\n",
    "    \n",
    "    #reshape the image\n",
    "\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state):\n",
    "    is_random = (random() < self.epsilon)\n",
    "    q_values = self.DQN.predict(state)\n",
    "    return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, actions, input_shape,\n",
    "                 minibatch_size=32,\n",
    "                 learning_rate=0.00025,\n",
    "                 discount_factor=0.99,\n",
    "                 dropout_prob=0.1,\n",
    "                 load_path=None,\n",
    "                 logger=None):\n",
    "\n",
    "        # Parameters\n",
    "        self.actions = actions  # Size of the network output\n",
    "        self.discount_factor = discount_factor  # Discount factor of the MDP\n",
    "        self.minibatch_size = minibatch_size  # Size of the training batches\n",
    "        self.learning_rate = learning_rate  # Learning rate\n",
    "        self.dropout_prob = dropout_prob  # Probability of dropout\n",
    "        self.logger = logger\n",
    "        self.training_history_csv = 'training_history.csv'\n",
    "\n",
    "        if self.logger is not None:\n",
    "            self.logger.to_csv(self.training_history_csv, 'Loss,Accuracy')\n",
    "\n",
    "        # Deep Q Network as defined in the DeepMind article on Nature\n",
    "        # Ordering channels first: (samples, channels, rows, cols)\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.model.add(Conv2D(32, 8, strides=(4, 4),\n",
    "                              padding='valid',\n",
    "                              activation='relu',\n",
    "                              input_shape=input_shape,\n",
    "                              data_format='channels_first'))\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.model.add(Conv2D(64, 4, strides=(2, 2),\n",
    "                              padding='valid',\n",
    "                              activation='relu',\n",
    "                              input_shape=input_shape,\n",
    "                              data_format='channels_first'))\n",
    "\n",
    "        # Third convolutional layer\n",
    "        self.model.add(Conv2D(64, 3, strides=(1, 1),\n",
    "                              padding='valid',\n",
    "                              activation='relu',\n",
    "                              input_shape=input_shape,\n",
    "                              data_format='channels_first'))\n",
    "\n",
    "        # Flatten the convolution output\n",
    "        self.model.add(Flatten())\n",
    "\n",
    "        # First dense layer\n",
    "        self.model.add(Dense(512, activation='relu'))\n",
    "\n",
    "        # Output layer\n",
    "        self.model.add(Dense(self.actions))\n",
    "\n",
    "        # Load the network weights from saved model\n",
    "        if load_path is not None:\n",
    "            self.load(load_path)\n",
    "\n",
    "        self.model.compile(loss='mean_squared_error',\n",
    "                           optimizer='rmsprop',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train(self, batch, DQN_target):\n",
    "        \"\"\"\n",
    "        Generates inputs and targets from the given batch, trains the model on\n",
    "        them.\n",
    "        :param batch: iterable of dictionaries with keys 'source', 'action',\n",
    "        'dest', 'reward'\n",
    "        :param DQN_target: a DQNetwork instance to generate targets\n",
    "        \"\"\"\n",
    "        x_train = []\n",
    "        t_train = []\n",
    "\n",
    "        # Generate training inputs and targets\n",
    "        for datapoint in batch:\n",
    "            # Inputs are the states\n",
    "            x_train.append(datapoint['source'].astype(np.float64))\n",
    "\n",
    "            # Apply the DQN or DDQN Q-value selection\n",
    "            next_state = datapoint['dest'].astype(np.float64)\n",
    "            next_state_pred = DQN_target.predict(next_state).ravel()\n",
    "            next_q_value = np.max(next_state_pred)\n",
    "\n",
    "            # The error must be 0 on all actions except the one taken\n",
    "            t = list(self.predict(datapoint['source'])[0])\n",
    "            if datapoint['final']:\n",
    "                t[datapoint['action']] = datapoint['reward']\n",
    "            else:\n",
    "                t[datapoint['action']] = datapoint['reward'] + \\\n",
    "                                         self.discount_factor * next_q_value\n",
    "            t_train.append(t)\n",
    "\n",
    "        # Prepare inputs and targets\n",
    "        x_train = np.asarray(x_train).squeeze()\n",
    "        t_train = np.asarray(t_train).squeeze()\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        h = self.model.fit(x_train,\n",
    "                           t_train,\n",
    "                           batch_size=self.minibatch_size,\n",
    "                           nb_epoch=1)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        if self.logger is not None:\n",
    "            self.logger.to_csv(self.training_history_csv,\n",
    "                               [h.history['loss'][0], h.history['acc'][0]])\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Feeds state to the model, returns predicted Q-values.\n",
    "        :param state: a numpy.array with same shape as the network's input\n",
    "        :return: numpy.array with predicted Q-values\n",
    "        \"\"\"\n",
    "        state = state.astype(np.float64)\n",
    "        return self.model.predict(state, batch_size=1)\n",
    "\n",
    "    def save(self, filename=None, append=''):\n",
    "        \"\"\"\n",
    "        Saves the model weights to disk.\n",
    "        :param filename: file to which save the weights (must end with \".h5\")\n",
    "        :param append: suffix to append after \"model\" in the default filename\n",
    "            if no filename is given\n",
    "        \"\"\"\n",
    "        f = ('model%s.h5' % append) if filename is None else filename\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Saving model as %s' % f)\n",
    "        self.model.save_weights(self.logger.path + f)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Loads the model's weights from path.\n",
    "        :param path: h5 file from which to load teh weights\n",
    "        \"\"\"\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Loading weights from file...')\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent:\n",
    "    def __init__(self,\n",
    "                 actions,\n",
    "                 network_input_shape,\n",
    "                 replay_memory_size=1024,\n",
    "                 minibatch_size=32,\n",
    "                 learning_rate=0.00025,\n",
    "                 discount_factor=0.9,\n",
    "                 dropout_prob=0.1,\n",
    "                 epsilon=1,\n",
    "                 epsilon_decrease_rate=0.99,\n",
    "                 min_epsilon=0.1,\n",
    "                 load_path=None,\n",
    "                 logger=None):\n",
    "\n",
    "        # Parameters\n",
    "        self.network_input_shape = network_input_shape  # Shape of the DQN input\n",
    "        self.actions = actions  # Size of the discrete action space\n",
    "        self.learning_rate = learning_rate  # Learning rate for the DQN\n",
    "        self.dropout_prob = dropout_prob  # Dropout probability of the DQN\n",
    "        self.load_path = load_path  # Path from which to load the DQN's weights\n",
    "        self.replay_memory_size = replay_memory_size  # Size of replay memory\n",
    "        self.minibatch_size = minibatch_size  # Size of a DQN minibatch\n",
    "        self.discount_factor = discount_factor  # Discount factor of the MDP\n",
    "        self.epsilon = epsilon  # Probability of taking a random action\n",
    "        self.epsilon_decrease_rate = epsilon_decrease_rate  # See update_epsilon\n",
    "        self.min_epsilon = min_epsilon  # Minimum value for epsilon\n",
    "        self.logger = logger\n",
    "\n",
    "        # Replay memory\n",
    "        self.experiences = []\n",
    "        self.training_count = 0\n",
    "\n",
    "        # Instantiate the deep Q-networks\n",
    "        # Main DQN\n",
    "        self.DQN = DQNetwork(\n",
    "            self.actions,\n",
    "            self.network_input_shape,\n",
    "            learning_rate=self.learning_rate,\n",
    "            discount_factor=self.discount_factor,\n",
    "            minibatch_size=self.minibatch_size,\n",
    "            dropout_prob=self.dropout_prob,\n",
    "            load_path=self.load_path,\n",
    "            logger=self.logger\n",
    "        )\n",
    "\n",
    "        # Target DQN used to generate targets\n",
    "        self.DQN_target = DQNetwork(\n",
    "            self.actions,\n",
    "            self.network_input_shape,\n",
    "            learning_rate=self.learning_rate,\n",
    "            discount_factor=self.discount_factor,\n",
    "            minibatch_size=self.minibatch_size,\n",
    "            dropout_prob=self.dropout_prob,\n",
    "            load_path=self.load_path,\n",
    "            logger=self.logger\n",
    "        )\n",
    "        # Reset target DQN\n",
    "        self.DQN_target.model.set_weights(self.DQN.model.get_weights())\n",
    "\n",
    "    def get_action(self, state, testing=False, force_random=False):\n",
    "        \"\"\"\n",
    "        Polls DQN for Q-values. Returns argmax(Q) with probability 1-epsilon\n",
    "        during training, 0.95 during testing.\n",
    "        :param state: a state that can be passed as input to DQN\n",
    "        :param testing: whether to use the current epsilon or the constant 0.05\n",
    "        :param force_random: whether to sample a random action regardless of\n",
    "            parameters\n",
    "        :return: the index of (action associated to) the highest Q-value \n",
    "        \"\"\"\n",
    "        is_random = (random() < (self.epsilon if not testing else 0.05))\n",
    "        if force_random or is_random:\n",
    "            return randint(0, self.actions - 1)\n",
    "        else:\n",
    "            q_values = self.DQN.predict(state)\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def get_max_q(self, state):\n",
    "        \"\"\"\n",
    "        Returns the maximum Q value predicted on the given state.\n",
    "        :param state: a state that can be passed as input to DQN\n",
    "        :return: an action index corresponding to the maximum Q-value in the \n",
    "            given state\n",
    "        \"\"\"\n",
    "        q_values = self.DQN.predict(state)\n",
    "        idxs = np.argwhere(q_values == np.max(q_values)).ravel()\n",
    "        return np.random.choice(idxs)\n",
    "\n",
    "    def get_random_state(self):\n",
    "        \"\"\"\n",
    "        Samples a random state from the replay memory.\n",
    "        :return: the sampled state\n",
    "        \"\"\"\n",
    "        return self.experiences[randrange(0, len(self.experiences))]['source']\n",
    "\n",
    "    def add_experience(self, source, action, reward, dest, final):\n",
    "        \"\"\"\n",
    "        Add a SARS' tuple to the experience replay.\n",
    "        :param source: source state\n",
    "        :param action: action index\n",
    "        :param reward: reward associated to the transition\n",
    "        :param dest: destination state\n",
    "        :param final: whether the state is absorbing\n",
    "        \"\"\"\n",
    "        # Remove older transitions if the replay memory is full\n",
    "        if len(self.experiences) >= self.replay_memory_size:\n",
    "            self.experiences.pop(0)\n",
    "        # Add a tuple (source, action, reward, dest, final) to replay memory\n",
    "        self.experiences.append({'source': source,\n",
    "                                 'action': action,\n",
    "                                 'reward': reward,\n",
    "                                 'dest': dest,\n",
    "                                 'final': final})\n",
    "        # Periodically log how many samples we've gathered so far\n",
    "        if (len(self.experiences) % 100 == 0) and (len(self.experiences) < self.replay_memory_size) and (self.logger is not None):\n",
    "            self.logger.log(\"Gathered %d samples of %d\" %\n",
    "                            (len(self.experiences), self.replay_memory_size))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"\n",
    "        Samples self.minibatch_size random transitions from the replay memory\n",
    "        and returns them as a batch.\n",
    "        :return: a batch of SARS' tuples\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        for i in xrange(self.minibatch_size):\n",
    "            batch.append(self.experiences[randrange(0, len(self.experiences))])\n",
    "        return np.asarray(batch)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the DQN on a minibatch of transitions.\n",
    "        \"\"\"\n",
    "        self.training_count += 1\n",
    "        print ('Training session #%d - epsilon: %f' % \\\n",
    "              (self.training_count, self.epsilon))\n",
    "        batch = self.sample_batch()\n",
    "        self.DQN.train(batch, self.DQN_target)  # Train the DQN\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decreases the probability of picking a random action, to improve\n",
    "        exploitation.\n",
    "        \"\"\"\n",
    "        if self.epsilon - self.epsilon_decrease_rate > self.min_epsilon:\n",
    "            self.epsilon -= self.epsilon_decrease_rate\n",
    "        else:\n",
    "            self.epsilon = self.min_epsilon\n",
    "\n",
    "    def reset_target_network(self):\n",
    "        \"\"\"\n",
    "        Updates the target DQN with the current weights of the main DQN.\n",
    "        \"\"\"\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Updating target network...')\n",
    "        self.DQN_target.model.set_weights(self.DQN.model.get_weights())\n",
    "\n",
    "    def quit(self):\n",
    "        \"\"\"\n",
    "        Saves the DQN and the target DQN to file.\n",
    "        \"\"\"\n",
    "        if self.load_path is None:\n",
    "            if self.logger is not None:\n",
    "                self.logger.log('Quitting...')\n",
    "            self.DQN.save(append='_DQN')\n",
    "            self.DQN_target.save(append='_DQN_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     DQA\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m      5\u001b[0m IMG_SIZE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m110\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mIMG_SIZE \u001b[38;5;241m=\u001b[39m IMG_SIZE\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# I/O\u001b[39;00m\n\u001b[0;32m      9\u001b[0m parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "def exit_handler():\n",
    "    global DQA\n",
    "    DQA.quit()\n",
    "\n",
    "IMG_SIZE = (84, 110)\n",
    "utils.IMG_SIZE = IMG_SIZE\n",
    "\n",
    "# I/O\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-t', '--train', action='store_true',\n",
    "                    help='train the agent')\n",
    "parser.add_argument('-l', '--load', type=str, default=None,\n",
    "                    help='load the neural network weights from the given path')\n",
    "parser.add_argument('-v', '--video', action='store_true',\n",
    "                    help='show video output')\n",
    "parser.add_argument('-d', '--debug', action='store_true',\n",
    "                    help='run in debug mode (no output files)')\n",
    "parser.add_argument('--eval', action='store_true',\n",
    "                    help='evaluate the agent')\n",
    "parser.add_argument('-e', '--environment', type=str,\n",
    "                    help='name of the OpenAI Gym environment to use '\n",
    "                         '(default: MsPacmanDeterministic-v4)',\n",
    "                    default='MsPacmanDeterministic-v4')\n",
    "parser.add_argument('--minibatch-size', type=int, default=32,\n",
    "                    help='number of sample to train the DQN at each update')\n",
    "parser.add_argument('--replay-memory-size', type=int, default=1e6,\n",
    "                    help='number of samples stored in the replay memory')\n",
    "parser.add_argument('--target-network-update-freq', type=int, default=10e3,\n",
    "                    help='frequency (number of DQN updates) with which the '\n",
    "                         'target DQN is updated')\n",
    "parser.add_argument('--avg-val-computation-freq', type=int, default=50e3,\n",
    "                    help='frequency (number of DQN updates) with which the '\n",
    "                         'average reward and Q value are computed')\n",
    "parser.add_argument('--discount-factor', type=float, default=0.99,\n",
    "                    help='discount factor for the environment')\n",
    "parser.add_argument('--update-freq', type=int, default=4,\n",
    "                    help='frequency (number of steps) with which to train the '\n",
    "                         'DQN')\n",
    "parser.add_argument('--learning-rate', type=float, default=0.00025,\n",
    "                    help='learning rate for optimizer')\n",
    "parser.add_argument('--epsilon', type=float, default=1,\n",
    "                    help='initial exploration rate for the agent')\n",
    "parser.add_argument('--min-epsilon', type=float, default=0.1,\n",
    "                    help='final exploration rate for the agent')\n",
    "parser.add_argument('--epsilon-decrease', type=float, default=9e-7,\n",
    "                    help='rate at which to linearly decrease epsilon')\n",
    "parser.add_argument('--replay-start-size', type=int, default=50e3,\n",
    "                    help='minimum number of transitions (with fully random '\n",
    "                         'policy) to store in the replay memory before '\n",
    "                         'starting training')\n",
    "parser.add_argument('--initial-random-actions', type=int, default=30,\n",
    "                    help='number of random actions to be performed by the agent'\n",
    "                         ' at the beginning of each episode')\n",
    "parser.add_argument('--dropout', type=float, default=0.,\n",
    "                    help='dropout rate for the DQN')\n",
    "parser.add_argument('--max-episodes', type=int, default=np.inf,\n",
    "                    help='maximum number of episodes that the agent can '\n",
    "                         'experience before quitting')\n",
    "parser.add_argument('--max-episode-length', type=int, default=np.inf,\n",
    "                    help='maximum number of steps in an episode')\n",
    "parser.add_argument('--max-frames-number', type=int, default=50e6,\n",
    "                    help='maximum number of frames during the whole algorithm')\n",
    "parser.add_argument('--test-freq', type=int, default=250000,\n",
    "                    help='frequency (number of frames) with which to test the '\n",
    "                         'agent\\'s performance')\n",
    "parser.add_argument('--validation-frames', type=int, default=135000,\n",
    "                    help='number of frames to test the model like in table 3 of'\n",
    "                         ' the paper')\n",
    "parser.add_argument('--test-states', type=int, default=30,\n",
    "                    help='number of states on which to compute the average Q '\n",
    "                         'value')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.debug:\n",
    "    print ('####################################################' \\\n",
    "          'WARNING: debug flag is set, output will not be saved' \\\n",
    "          '####################################################')\n",
    "\n",
    "logger = Logger(debug=args.debug, append=args.environment)\n",
    "atexit.register(exit_handler)  # Make sure to always save the model when exiting\n",
    "\n",
    "# Variables\n",
    "test_scores = []\n",
    "test_mean_q = []\n",
    "test_states = []\n",
    "\n",
    "# Setup\n",
    "env = gym.make(args.environment)\n",
    "network_input_shape = (4, 110, 84)  # Dimension ordering: 'th' (channels first)\n",
    "DQA = DQAgent(env.action_space.n,\n",
    "              network_input_shape,\n",
    "              replay_memory_size=args.replay_memory_size,\n",
    "              minibatch_size=args.minibatch_size,\n",
    "              learning_rate=args.learning_rate,\n",
    "              discount_factor=args.discount_factor,\n",
    "              dropout_prob=args.dropout,\n",
    "              epsilon=args.epsilon,\n",
    "              epsilon_decrease_rate=args.epsilon_decrease,\n",
    "              min_epsilon=args.min_epsilon,\n",
    "              load_path=args.load,\n",
    "              logger=logger)\n",
    "\n",
    "# Initial logging\n",
    "logger.log({\n",
    "    'Action space': env.action_space.n,\n",
    "    'Observation space': env.observation_space.shape\n",
    "})\n",
    "logger.log(vars(args))\n",
    "training_csv = 'training_info.csv'\n",
    "eval_csv = 'evaluation_info.csv'\n",
    "test_csv = 'test_score_mean_q_info.csv'\n",
    "logger.to_csv(training_csv, 'length,score')\n",
    "logger.to_csv(eval_csv, 'length,score')\n",
    "logger.to_csv(test_csv, 'avg_score,avg_Q')\n",
    "\n",
    "# Set counters\n",
    "episode = 0\n",
    "frame_counter = 0\n",
    "\n",
    "if args.train:\n",
    "    # Main loop\n",
    "    while episode < args.max_episodes:\n",
    "        # Start episode\n",
    "        logger.log(\"Episode %d\" % episode)\n",
    "        score = 0\n",
    "\n",
    "        # Observe reward and initialize first state\n",
    "        obs = utils.preprocess_observation(env.reset())\n",
    "\n",
    "        # Initialize the first state with the same 4 images\n",
    "        current_state = np.array([obs, obs, obs, obs])\n",
    "\n",
    "        # Main episode loop\n",
    "        t = 0\n",
    "        frame_counter += 1\n",
    "        while t < args.max_episode_length:\n",
    "            # Stop the episode if it takes too long\n",
    "            if frame_counter > args.max_frames_number:\n",
    "                DQA.quit()\n",
    "\n",
    "            # Render the game\n",
    "            if args.video:\n",
    "                env.render()\n",
    "\n",
    "            # Select an action using the DQA\n",
    "            action = DQA.get_action(np.asarray([current_state]))\n",
    "\n",
    "            # Observe reward and next state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            obs = utils.preprocess_observation(obs)\n",
    "            next_state = utils.get_next_state(current_state, obs)\n",
    "\n",
    "            frame_counter += 1\n",
    "\n",
    "            # Store transition in replay memory\n",
    "            clipped_reward = np.clip(reward, -1, 1)  # Clip the reward\n",
    "            DQA.add_experience(np.asarray([current_state]),\n",
    "                               action,\n",
    "                               clipped_reward,\n",
    "                               np.asarray([next_state]),\n",
    "                               done)\n",
    "\n",
    "            # Train the agent\n",
    "            if t % args.update_freq == 0 and len(DQA.experiences) >= args.replay_start_size:\n",
    "                DQA.train()\n",
    "                # Every C DQN updates, update DQN_target\n",
    "                if DQA.training_count % args.target_network_update_freq == 0 and DQA.training_count >= args.target_network_update_freq:\n",
    "                    DQA.reset_target_network()\n",
    "                # Log the mean score and mean Q values of test states\n",
    "                if DQA.training_count % args.avg_val_computation_freq == 0 and DQA.training_count >= args.avg_val_computation_freq:\n",
    "                    logger.to_csv(test_csv,\n",
    "                                  [np.mean(test_scores), np.mean(test_mean_q)])\n",
    "                    del test_scores[:]\n",
    "                    del test_mean_q[:]\n",
    "\n",
    "            # Linear epsilon annealing\n",
    "            if len(DQA.experiences) >= args.replay_start_size:\n",
    "                DQA.update_epsilon()\n",
    "\n",
    "            # Update the current state and score\n",
    "            current_state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # Log episode data in the training csv\n",
    "            if done or t == args.max_episode_length - 1:\n",
    "                logger.to_csv(training_csv, [t, score])\n",
    "                logger.log(\"Length: %d; Score: %d\\n\" % (t + 1, score))\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            # Evaluate the agent's performance\n",
    "            if frame_counter % args.test_freq == 0:\n",
    "                t_evaluation, score_evaluation = evaluate(DQA, args, logger)\n",
    "                # Log evaluation data\n",
    "                logger.to_csv(eval_csv, [t_evaluation, score_evaluation])\n",
    "\n",
    "            # Hold out a set of test states to monitor the mean Q value\n",
    "            if len(test_states) < args.test_states:\n",
    "                # Generate test states\n",
    "                for _ in range(random.randint(1, 5)):\n",
    "                    test_states.append(DQA.get_random_state())\n",
    "            else:\n",
    "                # Update scores and mean Q values\n",
    "                test_scores.append(score)\n",
    "                test_q_values = [DQA.get_max_q(state) for state in test_states]\n",
    "                test_mean_q.append(np.mean(test_q_values))\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "if args.eval:\n",
    "    logger.log(evaluate(DQA, args, logger))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
