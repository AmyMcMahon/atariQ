{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dervla Gargan - 22346279\n",
    "Mark Langtry - 22340475\n",
    "Amy McMahon - 22346619\n",
    "\n",
    "Code executed without errors :)\n",
    "\n",
    "**Links used**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.51963544,  0.        ], dtype=float32), {})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = env.observation_space\n",
    "actions = env.action_space\n",
    "EPSILON = 1\n",
    "NUM_ACTIONS = actions.n\n",
    "LEARNING_RATE = 0.001  # Adjust as needed\n",
    "DISCOUNT_FACTOR = 0.99  # Adjust as needed\n",
    "NUM_EPISODES = 1000\n",
    "replay_buffer_size = 10000  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "update_target_network_freq = 100  # Adjust as needed\n",
    "rewards_per_episode = []\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = 1 / NUM_EPISODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(24, activation='relu', input_shape=state_shape),\n",
    "    tf.keras.layers.Dense(48, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_ACTIONS, activation='linear')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='mse')\n",
    "\n",
    "target_model = tf.keras.models.clone_model(model)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting velocity and position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.linspace(space.low[0], space.high[0], 20)\n",
    "velocity = np.linspace(space.low[1], space.high[1], 20)\n",
    "\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(position, velocity, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(NUM_ACTIONS)\n",
    "    else:\n",
    "        q_values = model.predict(np.array([[position, velocity]]))[0]\n",
    "        return np.argmax(q_values)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def epsilon_greedy_policy(state, epsilon=0):\n",
    "#     print(state)\n",
    "#     if np.random.rand() >= epsilon:\n",
    "#         return np.random.randint(n_outputs)\n",
    "#     else:\n",
    "#         state_array = state[0]\n",
    "#         Q_values = model.predict(state_array[np.newaxis], verbose=0)[0]\n",
    "\n",
    "#         return Q_values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_one_step(env, state, epsilon):\n",
    "#     action = epsilon_greedy_policy(state, epsilon)\n",
    "#     next_state, reward, done, info = env.step(action)\n",
    "#     replay_buffer.append((state, action, reward, next_state, done))\n",
    "#     return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset(seed=42)\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "# rewards = []\n",
    "# best_score = 0\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay_buffer = deque(maxlen=400)\n",
    "\n",
    "# def sample_experiences(batch_size):\n",
    "#     indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "#     batch = [replay_buffer[index] for index in indices]\n",
    "#     return [\n",
    "#         np.array([experience[field_index] for experience in batch])\n",
    "#         for field_index in range(5)\n",
    "#     ]  # [states, actions, rewards, next_states, dones, truncateds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "# lossFunc = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# def training(batch_size):\n",
    "#     experiences = sample_experiences(batch_size)\n",
    "#     states, actions, rewards, next_states, dones = experiences\n",
    "#     next_Q_values = model.predict(next_states, verbose=0)\n",
    "#     min_next_Q_values = next_Q_values.min(axis=1)\n",
    "#     runs = 1.0 - (dones)\n",
    "#     target_Q_values = rewards + runs * discount_factor * min_next_Q_values\n",
    "#     target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "#     mask = tf.one_hot(actions, n_outputs)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         all_Q_values = model(states)\n",
    "#         Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "#         loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "#     grads = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    state = env.reset()\n",
    "    state_position = np.digitize(state[0], position)\n",
    "    state_velocity = np.digitize(state[1], velocity)\n",
    "\n",
    "    goal = False\n",
    "    rewards = 0\n",
    "\n",
    "    while not goal and rewards > -1000:\n",
    "        action = select_action(state_position, state_velocity, EPSILON)\n",
    "        next_state, reward, goal, _, _ = env.step(action)\n",
    "        next_state_position = np.digitize(next_state[0], position)\n",
    "        next_state_velocity = np.digitize(next_state[1], velocity)\n",
    "\n",
    "        replay_buffer.append((state_position, state_velocity, action, reward, next_state_position, next_state_velocity, goal))\n",
    "\n",
    "        state_position = next_state_position\n",
    "        state_velocity = next_state_velocity\n",
    "        rewards += reward\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = np.array(random.sample(replay_buffer, batch_size))\n",
    "            states = np.array([(s[0], s[1]) for s in batch[:, :2]])\n",
    "            next_states = np.array([(s[4], s[5]) for s in batch[:, :2]])\n",
    "\n",
    "            target_q_values = model.predict(states)\n",
    "            next_q_values_target = target_model.predict(next_states)\n",
    "            for i, (state_position, state_velocity, action, reward, next_state_position, next_state_velocity, done) in enumerate(batch):\n",
    "                if not done:\n",
    "                    target_q_values[i][action] = reward + DISCOUNT_FACTOR * np.amax(next_q_values_target[i])\n",
    "                else:\n",
    "                    target_q_values[i][action] = reward\n",
    "\n",
    "            model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "\n",
    "        if len(replay_buffer) > batch_size and episode % update_target_network_freq == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "    rewards_per_episode.append(rewards)\n",
    "    global EPSILON\n",
    "    EPSILON = max(EPSILON - EPSILON_DECAY, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.5122243,  0.       ], dtype=float32), {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m      4\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     obs, reward, done, info  \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m, in \u001b[0;36mplay_one_step\u001b[1;34m(env, state, epsilon)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_one_step\u001b[39m(env, state, epsilon):\n\u001b[0;32m      2\u001b[0m     action \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(state, epsilon)\n\u001b[1;32m----> 3\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      4\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mappend((state, action, reward, next_state, done))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_state, reward, done, info\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# for episode in range(300):\n",
    "#     obs = env.reset()\n",
    "#     for step in range(200):\n",
    "#         epsilon = 0.5\n",
    "#         obs, reward, done, info  = play_one_step(env,obs, epsilon)\n",
    "#         if done:\n",
    "#             break\n",
    "#     print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}\",\n",
    "#           end=\"\")\n",
    "#     rewards.append(step)\n",
    "#     if step >= best_score:\n",
    "#         best_weights = model.get_weights()\n",
    "#         best_score = step\n",
    "\n",
    "#     if episode > 50:\n",
    "#         training_step(batch_size)\n",
    "\n",
    "# model.set_weights(best_weights)\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    env.render()\n",
    "    train()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(EPSILON, EPSILON_DECAY):\n",
    "#     state = env.reset()[0]\n",
    "#     state_position = np.digitize(state[0], position)\n",
    "#     state_velocity = np.digitize(state[1], velocity)\n",
    "\n",
    "#     goal = False\n",
    "\n",
    "#     rewards = 0\n",
    "\n",
    "#     while not goal and rewards > -1000:\n",
    "#         action = select_action(state)\n",
    "#         next_state, reward, goal, _, _ = env.step(action)\n",
    "#         next_state_position = np.digitize(next_state[0], position)\n",
    "#         next_state_velocity = np.digitize(next_state[1], velocity)\n",
    "#         max_q_value_next_state = np.max(q_values[next_state_position][next_state_velocity])\n",
    "#         q_values[state_position][state_velocity][action] += LEARNING_RATE * (reward + DISCOUNT_FACTOR * \n",
    "#                                 max_q_value_next_state - q_values[state_position][state_velocity][action])\n",
    "#         state_position = next_state_position\n",
    "#         state_velocity = next_state_velocity\n",
    "\n",
    "#         rewards += reward\n",
    "\n",
    "#     rewards_per_episode.append(rewards)\n",
    "#     NEW_EPSILON = max(EPSILON - EPSILON_DECAY, 0)\n",
    "#     return NEW_EPSILON\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(NUM_EPISODES):\n",
    "#     EPSILON = train(EPSILON, EPSILON_DECAY)\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_rewards = np.zeros(NUM_EPISODES)\n",
    "# for t in range(NUM_EPISODES):\n",
    "#     mean_rewards[t] = np.mean(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "# plt.plot(mean_rewards)\n",
    "mean_rewards = np.zeros(NUM_EPISODES)\n",
    "for t in range(NUM_EPISODES):\n",
    "    mean_rewards[t] = np.mean(rewards_per_episode[max(0, t - 100):(t + 1)])\n",
    "plt.plot(mean_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Mean Rewards per Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
