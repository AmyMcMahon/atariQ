{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dervla Gargan - 22346279\n",
    "Mark Langtry - 22340475\n",
    "Amy McMahon - 22346619\n",
    "\n",
    "Code executed without errors :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, actionSpace, stateSpace):\n",
    "\n",
    "        self.actionSpace = actionSpace\n",
    "        self.stateSpace = stateSpace\n",
    "        self.epsilon = 1.0 #\n",
    "        self.gamma = .95\n",
    "        self.batchSize = 64\n",
    "        self.epsilonMin = .01 #\n",
    "        self.lr = 0.001\n",
    "        self.epsilonDecay = .995 #\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.stateSpace, activation=relu))\n",
    "        model.add(Dense(24, activation=relu))\n",
    "        model.add(Dense(self.actionSpace, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state): # epsilon greedy policy\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.actionSpace)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            return\n",
    "\n",
    "        smallBatch = random.sample(self.memory, self.batchSize)\n",
    "        sb_states = np.array([i[0] for i in smallBatch])\n",
    "        sb_actions = np.array([i[1] for i in smallBatch])\n",
    "        sb_rewards = np.array([i[2] for i in smallBatch])\n",
    "        sb_nextStates = np.array([i[3] for i in smallBatch])\n",
    "        sb_dones = np.array([i[4] for i in smallBatch])\n",
    "\n",
    "        sb_states = np.squeeze(sb_states)\n",
    "        sb_nextStates = np.squeeze(sb_nextStates)\n",
    "\n",
    "        targets = sb_rewards + self.gamma*(np.amax(self.model.predict_on_batch(sb_nextStates), axis=1))*(1-sb_dones)\n",
    "        targets_full = self.model.predict_on_batch(sb_states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batchSize)])\n",
    "        targets_full[[ind], [sb_actions]] = targets\n",
    "\n",
    "        self.model.fit(sb_states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilonMin:\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Reward for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state):\n",
    "\n",
    "    if state[0] >= 0.5:\n",
    "        print(\"The car has sucessfully reached the goal\")\n",
    "        return 10\n",
    "    if state[0] > -0.4:\n",
    "        return (1+state[0])**2\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to train the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_dqn(episode):\n",
    "\n",
    "    lossList = []\n",
    "    dqn_agent = DQN(env.action_space.n, env.observation_space.shape[0])\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.array(state[0])\n",
    "        state = np.reshape(state, (1, dqn_agent.stateSpace))\n",
    "        score = 0\n",
    "        max_steps = 100\n",
    "        for i in range(max_steps):\n",
    "            action = dqn_agent.act(state)\n",
    "            env.render()\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = get_reward(next_state)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, dqn_agent.stateSpace))\n",
    "            dqn_agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            dqn_agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        lossList.append(score)\n",
    "    return lossList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_policy(episode, step):\n",
    "\n",
    "    for i_episode in range(episode):\n",
    "        env.reset()\n",
    "        for t in range(step):\n",
    "            env.render()\n",
    "            action = env.actionSpace.sample()\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                print(\"The Episode finished running after {} timesteps\".format(t+1))\n",
    "                break\n",
    "            print(\"Beginning the next epsiode\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    numOfEpisodes = 60\n",
    "    loss = train_dqn(numOfEpisodes)\n",
    "    plt.plot([i+1 for i in range(episodes)], loss)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
